\section{Algorithms}

\subsection{Total variation}
In this section we take a look at the main results from \citep{hazan}.
Specifically we present the main algorithms used in the paper and review 
how the authors were able to bound the regret of their algorithm in terms 
of the total variation.

\subsubsection{Algorithms}
First we state the algorithm developed in \citep{hazan} in full. The
main algorithm is composed of two prominent steps that are alternated with 
some probability. This algorithm uses reservoir sampling and the 
properties of Dikin-ellipsoids to efficiently perform online bandit linear
optimization that can then be bounded in terms of total variation.

%algorithms 
\input{bbb_mainalgo.tex}
\unskip
\input{bbb_simplexsample.tex}
\unskip
\input{bbb_ellipsoidsample.tex}
\unskip

%descriptions
Now that we have formally stated the algorithm of \citep{hazan} and its main
components, let us step through each part of the algorithm and go into a 
bit more detail. We first examine the overall algorithm, then the 
SIMPLEXSAMPLE step and then finally the ELLIPSOIDSAMPLE step. We
give intuition when applicable and provide mathematical justification when
it adds to the conceptual understanding (as opposed to being primarily technical).

\textbf{Main Algorithm}
We start at the top level algorithm \textit{bandit online linear optimization}.

The algorithm has three main parts: calling SIMPLEXSAMPLE with some probability, calling ELLIPSOID-SAMPLE with the remaining probability mass and updating the value $x_t$ at every round. At a high level the SIMPLEXSAMPLE step is an exploration step, and the ELLIPSOIDSAMPLE step is an exploration and exploitation step.

We see on lines 1-4 some initialization of the various quantities involved. Seen listed are the parameters of the algorithm $\eta$: the exploration/exploitation trade off rate $\mathcal{V}$ self concordant $\mathcal{R}$ and a size parameter $k$ which denotes how large of a reservoir we are willing to keep for each of our coordinates (arms). We also initialize $x$ to be the index to the minimum point in $\mathcal{R}(x)$  and the estimate for the mean reward of each hand to be zero to start.

We then take some action at some time step $t$ and do this for until the last time step $T$. At each round we have the choice of exploring alone with a SIMPLEXSAMPLE or exploring and exploiting with an $ELLIPSOIDSAMPLE$ step. We let the proportion of time spent in either of these tasks  be determined by the parameter $\eta$ and the stage of the optimization as denoted by t. We also take into account the size of our reservoir $k$ because of how it effects the accuracy of our estimates of the mean losses being estimated. More generally we let the proportion of time we spend exploring grow smaller as time goes on (time spent in a SIMPLEXSAMPLE) and the proportion of time spent exploring and exploiting go up in 
later stages (time spent in ELLIPSOIDSAMPLE).

%TODO clarify what the $i_t$ step is doing 

We can see from the algorithm that our estimate of the mean $\tilde{\mu}$ of each coordinate as computed using reservoir sampling only gets updated during the SIMPLEXSAMPLE procedure, but only gets used in the ELLIPSOIDSAMPLE procedure. This matches our intuition that exploiting is akin to using the information that we have collected regarding the losses of each arm and exploring involves gathering this information. We also see that $\tilde{f_t}$ is not updated when a SIMPLEXSAMPLE is taken, only when an ELLIPSOIDSAMPLE is taken. 
%TODO clarify why the f_t is not updated in more detail


%Describe the SIMPLEXSAMPLE procedure
\textbf{SIMPLEXSAMPLE}
Now we take a look at the SIMPLEXSAMPLE procedure whose job is simply to implement reservoir sampling on all the points in the feasible set with the given reservoir size. SIMPLEXSAMPLE samples a random coordinate $i \in [n]$ uniformly, the actual sampled point  $y_t$ is the corresponding vertex $\gamma e_{i_t}$. This vertex is of the scaled n - dimensional simplex and by assumption it has to be contained inside of $\mathcal{K}$. The loss is immediately received as $f_t(i_t)$. We then do reservoir sampling: if one of the slots in our reservoir for that coordinate is empty then we put our loss in that slot, otherwise we kick one element out of the reservoir and put the new loss there. The point kicked out is done so at random uniformly. This procedure exactly implements reservoir sampling and guarantees an unbiased estimate of the mean loss for that coordinate in the limit. We return the entire vector of means to be used in the main algorithm. Every element of that vector is the estimate of the mean of the coordinate denoted by its index, at the current round t.
 
%Describe the ELLIPSOID-SAMPLE PROCEDURE
\textbf{ELLIPSOID-SAMPLE}
ELLIPSOIDSAMPLE is where we exploit our knowledge of the adversary (loss surface) by using our estimate of the mean. This is a modification of a similar procedure outlined in \citep{abernethy}. \citep{abernethy} manages to  prove that this style of sampling procedure is unbiased and has low variation with respect to the regularization which in this case are our self-concordant functions. We choose a point $y_t$ just like in SIMPLEXSAMPLE but it is  chosen from the endpoints of the principal axes of the Dikin ellipsoid $W_1(x_t)$ centered at $x_t$ this time. Recall that $x_t$ was constructed to minimize the FTRL loss $\eta \sum_{\tau=1}^T\tilde{f_t^T} +\mathcal{R}$. Since the loss minimized was in terms of our \textit{estimate} of the loss vector, this is where we are exploiting our prior knowledge of the losses associated with the coordinates. Furthermore, we update this estimate of $\tilde{f_t}$ at the end of ELLIPSOIDSAMPLE to incorporate our knowledge of the mean $\tilde{\mu_t}$ and our actually loss suffered $f_t$, which is done by adding $f_t$ to  $\eta (f_{t}^Ty_t - \tilde{\mu_{t}^T}y_t)\epsilon \lambda_{it}^{\frac{1}{2}}v_{it}$ .

The procedure is actually quite akin to the exploration-exploitation steps taken in FTRl or EXP3, but there is a lot of mathematical machinery needed to ensure that our sampled point $y_t$ will be in the feasible set and that our ellipsoid sampling procedure has nice properties that ensure correctness and efficiency. 


\subsection{Partial information setting}
As mentioned in the introduction, the authors in \citep{alon} set about exploring how semantic connections in the action set can influence the bandit model. In particular, they argue that similarities between actions can tighten the regret bound of the algorithm. For instance, in the case of website advertising, they hypothesis that if a user clicks on one ad for running shoes, then the user might be susceptible to click on other ads for running shoes. In contrast, if the user is not interested in one pair of running shoes, chances are that the user is not interested in running shoes at all. Thus, there exists a potential for learning something about the category of running shoes. 

The authors cleverly model these semantic relationships in a \textit{feedback graph} $G_t=(\mathcal{K},D_t)$ where $\mathcal{K}$ is the set of actions and $D_t$ the set of arcs. Let any arc $(i,j)\in D_t$ for $i\not j$ if and only if playing action $i\in\mathcal{K}$ reveals the loss of action $j\in \mathcal{K}$ at time $t$. The full information setting then corresponds to a complete graph; the bandit setting, an empty graph. Furthermore, any subgraph of $G$ represents the partial information setting which is a direct interpolation between the expert and the bandit setting. In order to prove guarantees in this scenario, the authors examine some graph theoretic notions including, \textit{independence numbers}, \textit{dominating sets}, and \textit{maximum acyclic graphs}. Recall that the independence number, $\alpha(G)$, is the cardinality of the maximal subset $T\subset \mathcal{K}$ such that no two $i,j\in T$ are connected by an edge; that is $(i,j)\not\in D_t$. An independent set $T$ is maximal if no proper superset of $T$ forms an independent set. Moreover, $R\subset G$ is a dominating set if for any $j\not\in R$ there exists an $i\in R$ that exhibits a directed edge to $j$. Lastly, the maximum acyclic subgraph in $G$, denoted $\mas(G)$, is the largest subgraph in $G$ with no directed cycles. 

Based on these formal structures, the authors analyze and give regret bounds on modified versions of the Exp3 algorithm. A key concept in their analysis is the central quantity:

\begin{align}
	Q_t=\sum_{i\in |\mathcal{K}|}\frac{p_{i,t}}{q_{i,t}} = \sum_{i\in |\mathcal{K}|}\frac{p_{i,t}}{\sum_{j:j\overset{t}{\to}j} p_{j,t}}.
\end{align}

In the quest of bounding $Q_t$, they provide the following regret bound

$$R_t\leq O(\sqrt{\log |\mathcal{K}|\sum_{t=1}^T\mas(G_t)}$$

for a directed graph in the uninformed setting, that is the feedback graph is revealed to the learner after the action is taken. If the feedback graph is undirected, $\mas(G_t)$ is replaced by $\alpha(G)$. In the informed setting where the feedback graph is known ahead of time, 

$$R_t \leq O(\log K\sqrt{\log KT\sum_{t=1}^T\alpha(G_t)}$$

\subsection{LP relaxations}

\subsubsection{Overview of results}
The main contribution provided by \citep{bertsimas} is to provide a hierarchy of LP-relaxations
to approximately solve the restless bandits problem. Earlier we stated what the restless
bandits problem consists of and the key quantities of interest. Here we first describe the ideas leading up to, and influencing, these results. We then formally state the LP-relaxation developed by \citep{bertsimas} for the restless bandits including pertinent quantities and relationships. Then we summarize experimental results provided by the authors before finishing with a discussion of theoretical findings and computational concerns.

\subsubsection{Related ideas and motivating work}
The results provided by \citep{bertsimas} directly draw inspiration from previous work and ideas in combinatorial optimization, stochastic optimization and attempts to formulate LP-relaxations for the restless bandits approach that have been undertaken before. \\ 


A first order relaxed version of the restless bandits problem was introduced by \citep{whittle}. This version could be solved in polynomial time. There were many restrictions and assumptions made by the approach which makes it less than ideal. Whittle also proposed a heuristic that relaxes the problem into one solvable in polynomial time. This heuristic, however only applies to a restricted class of bandits and is not as general as those developed here.\\

Another motivating result was the finding by \citep{papad}that the general restless bandits is PSPACE-hard. This  motivates an approximate solution rather than trying to find an exact one which is infeasible in general. \\ 

\subsubsection{Formulation and relaxation}

We now show how \citep{bertsimas} phrase the restless bandit problem in the language of Linear Programs and thereby procure a relaxation to the latter type of problem. They first describe performance measures for the problem.

Let the state-action space be  $\ell= \{(i,a): i \in \mathcal{L}, a \in  \mathcal{A}_i \}$
with states $i$ and actions $a$. Then the following performance measures can be introduced :

$x_j(u) = E_u[\sum_{t=0}^{\infty}I_j^a(t) \beta^t]$ where $I_j^a = 1$ if an action is taken at the time step t in the state j, and 0 otherwise. 

The optimization process that follows from the performance measure introduced is as follows 
$Z^* = max_{u \in \mathcal{U}} \sum_{(i,a) \in \mathcal{L}} R_j^a x_j^a(u)$.  

Finally they show the explicit performance region spanned by the vectors $x(u)$ under all the policies that are in consideration. This leads to a mathematical program : 

$Z^* = max_{x \in \mathcal{X}}\sum_{(i,a)\in \mathcal{L}}R_i^a x_i^a$ where $x = (x_j^a)$. 

As is typical of most linear programming formulations, the feasible set lives on a polytope of some type, in this case they live in the following polyhedron $\mathcal {P}$ 

$\mathcal{P} = \{ x \in R_+^{|\mathcal{L}|} \}: \sum_{a \in \mathcal{A}} x_j^a = \alpha_j 
+ \beta \sum_{(i,a) \in \mathcal{L}} p_{i,j}^a x_i^a, j  \in \mathcal{L}$ 

Next the authors present the crucial argument that leads to an LP formulation of the problem.
They present a known theorem that connects the polytope and the performance region.

The following statements hold
\begin{equation}
(a) X = \mathcal{P}
(b)\text{the vertices of the polytope } \mathcal{P}
\end{equation} 

are achievable by stationary deterministic policies

This leads to the formal formulation of the restless bandit problem as an LP.

First take the following two performance measures:
$x_{i_n}^1 = E_u[\sum_{t=0}^{\infty} I_{i_n}(t)\beta^t]$ 

$x_{i_n}^0(u) = E_u[\sum_{t=0}^{\infty}I_{i_n}(t) \beta^t]$ where $u$ is an admissible scheduling policy.  \\

$I_{i_n}^1(t)$ is 1 if project n is in state $i_n$and active at time t and 0 otherwise.

$I_{i_n}^0(t)$ is 1 if the project n is in state $i_n$ and passive at time t and 0 otherwise.

Thus we give one performance measure to the current set of active tasks and one for the current set of passive tasks. To combine them and get a performance measure for the entire region would give : 


$X = \{x = (x_{i_n}^{a_n}(u))_{i_n \in \mathcal{L}_n, a_n \in \{0,1\}, n \in \mathcal{N}|u \in \mathcal{U}}\}$\\

From the theorem shown earlier we know that there is a correspondence between a discounted MDC and a polytope which means we can take the performance measures and create a linear program associated to that polytope.

$(LP) Z* = max_{x \in \mathcal{X}} \sum_{n \in \mathcal{N}} \sum_{i_n \in \mathcal{L}_n} 
\sum_{a_n \in \{ 0m1\} R_{i_n}^{a_n} x_{i_n}^{a_n}} $

Thus we get a general formulation for a linear program associated to the restless bandit problems by phrasing the discounted MDC formulation as a polytope with its associated linear program. 
