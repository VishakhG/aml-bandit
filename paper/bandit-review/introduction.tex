\section{Introduction}

The multi-armed bandit scenario is synonymous to a class of computer science problems in which an online learner seeks to minimise her regret over a number of actions. In particular to the bandit setting, at each round $t$ only the loss of the given action is disclosed to the learner. The learner thus finds herself in a partial information setting where learning becomes more difficult as reflected in the increase of in the optimal algorithmic bound from $\sqrt{\ln(K)T}$ in the full information setting to $\sqrt{KT}$ with $K$ representing the number actions available to the learner at each round; $T$, the total rounds. 

In this paper, we review the findings of the papers \citep{alon, hazan, bertsimas}. Common to all of the papers, the authors expand on the analytical body of work related to the multi-armed bandit problem. In particular, \citep{alon} considers the adversarial bandit problem in which the loss of several actions are revealed based on a feedback graph at each round. \citep{hazan} develops a series of linear programming relaxations for approximately solving restless bandit problem. Lastly, \citep{bertsimas} bounds the regret in the non-stochastic bandit setting by the total variation in the loss functions. In the following, we first expound on the problem areas of the papers. Subsequently, we introduce the papers' novel conceptual ideas before backing them up with their respective analyses. Lastly, we provide a simple implementation of ... before rounding off with concluding remarks.