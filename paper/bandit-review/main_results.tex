\section{Overview of main results}

\subsection{Total variation}
In this section we go into the algorithms and techniques used in \citep{hazan}
in more detail to explore how they managed to bound regret by total variation in the
adversarial bandit setting.

\subsubsection{Algorithms}
First we state the algorithm developed in \citep{hazan} in full, and walk through them
to analyze the choices that were made.

%algorithms 
\input{bbb_mainalgo.tex}
\input{bbb_simplexsample.tex}
\input{bbb_ellipsoidsample.tex}

%descriptions
Now that we have formally stated the algorithms, let's try and understand
what each one does and the main ideas being employed. 

\textbf{Main Algorithm}
Naturally we start at the top level algorithm \textit{bandit online linear optimization}.
The algorithm has three main parts: calling SIMPLEX-SAMPLE occasionally, calling ELLIPSOID-SAMPLE
occasionally and updating $x_t$ at every round. \\

We see on lines 1-4 some initialization of the various variables involved. 
We take as parameters $\eta, \mathcal{V} self concordant \mathcal{R}$ and a size 
parameter $k$. We also initialize x to be the index to the minimum point in $\mathcal{R}(x)$ 
and the estimate for the mean reward of each hand to be zero. \\

We then loop through all the rounds. At each round we have the choice of exploring alone 
with a $SIMPLEX-SAMPLE$ or exploring and exploiting with an $ELLIPSOID-SAMPLE$. We 
let the proportion of time spent in either of these helper functions be determined by 
the parameter $\eta$, the size of our reservoir $k$ and which stage we are in with respect to
the optimization procedure as denoted by $t$. Specifically we let the proportion of time 
we spend exploring grow smaller as time goes on (time spent in a SIMPLEX-SAMPLE) and the proportion of time spent exploring and exploiting go up in later stages (time spent in ELLIPSOID-SAMPLE).\\

%TODO clarify what the $i_t$ step is doing 

We can see from the algorithm that our estimate of the mean $\tilde{mu}$ of each arm as computed 
using reservoir sampling only gets updated during the SIMPLEXSAMPLE procedure, but only gets 
used in the ELLIPSOIDSAMPLE procedure. This matches our intuition that exploiting means using the
information that we have collected regarding the losses of each arm and exploring involves gathering this statistic. We also see that $\tilde{f_t}$ is not updated when a SIMPLEXSAMPLE is taken,
only when an ELLIPSOIDSAMPLE is taken. \\
%TODO clarify why the f_t is not updated in more detail


%Describe the SIMPLEX-SAMPLE procedure
\textbf{SIMPLEXSAMPLE}
Now we take a look at the SIMPLEXSAMPLE procedure whose job is simply to perform reservoir sampling on all the points in the feasible set with the given reservoir size.It does this when called, which as discussed earlier is a fraction of time $\frac{nk}{t}$ that decreases with time.
SIMPLEXSAMPLE samples a random point $i \in [n]$ uniformly, the actual sampled point (arm) 
is the corresponding vertex $\gamma e_{i_t}$. This vertex is of the scaled n-dimensional simplex
and by assumption it has to be contained inside of $\mathcal{K}$. The loss is immediately received as $f_t(i_t)$. We then do reservoir sampling: if one of the slots in our reservoir for that
point is empty then we put our loss in that slot, otherwise we kick one element out of the reservoir and put the new loss there. The point kicked out is done so at random uniformly. This procedure exactly implements reservoir sampling and guarantees an unbiased estimate of the mean loss
for that coordinate in the limit. We return the entire vector of means to be used in the main algorithm. Every element of that vector is the estimate of the mean of the coordinate denoted by its
index, at the current round t.
 
%Describe the ELLIPSOID-SAMPLE PROCEDURE
\textbf{ELLIPSOID-SAMPLE}
ELLIPSOIDSAMPLE is where we exploit our knowledge of the adversary (loss surface) by using
our estimate of the mean. This is a modification of a similar procedure outlined in \citep{abernethy}. The paper referenced proves that this sampling procedure is unbiased and has low variation with respect to the regularization. We choose a point again $y_t$ but its chosen from the endpoints of the principal axes of the Dikin ellipsoid $W_1(x_t)$ centered at $x_t$. Recall that our $x_t$ was constructed to minimize the FTRL loss $\eta \sum_{\tau=1}^T\tilde{f_t^T} +\mathcal{R}$.
Since the loss minimized was in terms of our estimate of the loss vector, this is where we are
exploiting our experience with the losses of our coordinates. Furthermore, we update this estimate of $\tilde{f_t}$ at the end of ELLIPSOIDSAMPLE to incorporate our knowledge of the mean $\tilde{\mu_t}$ and our actually loss suffered $f_t$, which is done by adding $f_t$ to 
$\eta (f_{t}^Ty_t - \tilde{\mu_{t}^T}y_t)\epsilon \lambda_{it}^{\frac{1}{2}}v_{it}$ .\\

The procedure is actually quite simple, but there is a lot of mathematical machinery needed to
ensure that our sampled point $y_t$ will be in the feasible set and that our ellipsoid sampling
procedure has nice properties that ensure correctness and efficiency. 

\subsubsection{Guarantees}
\input{bbb_proof.tex}


\subsection{Partial information setting}
As mentioned in the introduction, the authors in \citep{alon} set about exploring how semantic connections in the action set can influence the bandit model. In particular, they argue that similarities between actions can tighten the regret bound of the algorithm. For instance, in the case of website advertising, they hypothesise that if a user clicks on one ad for running shoes, then the user might be susceptible to click on other ads for running shoes. In contrast, if the user is not interested in one pair of running shoes, chances are that the user is not interested in running shoes at all. Thus, there exists a potential for learning something about the category of running shoes. 

The authors clervely model these semantic relationships in a \textit{feedback graph} $G_t=(\mathcal{K},D_t)$ where $\mathcal{K}$ is the set of actions and $D_t$ the set of arcs. Let any arc $(i,j)\in D_t$ for $i\not j$ if and only if playing action $i\in\mathcal{K}$ reveals the loss of action $j\in \mathcal{K}$ at time $t$. The full information setting then corresponds to a complete graph; the bandit setting, an empty graph. Furthermore, any subgraph of $G$ represents the partial information setting which is a direct interpolation between the expert and the bandit setting. In order to prove guarantees in this scenario, the authors examine some graph theoretic notions including, \textit{independence numbers}, \textit{dominating sets}, and \textit{maximum acyclic graphs}. Recall that the independence number, $\alpha(G)$, is the cardinality of the maximal subset $T\subset \mathcal{K}$ such that no two $i,j\in T$ are connected by an edge; that is $(i,j)\not\in D_t$. An independent set $T$ is maximal if no proper superset of $T$ forms an independent set. Moreover, $R\subset G$ is a dominating set if for any $j\not\in R$ there exists an $i\in R$ that exhibits a directed edge to $j$. Lastly, the maximum acyclic subgraph in $G$, denoted $\mas(G)$, is the largest subgraph in $G$ with no directed cycles. Based on these formal structures, the authors analyse and give regret bounds on modified versions of the Exp3 algorithm. 

In the uninformed setting, when the feedback graph is hidden from the learner until an action is taken, they obtain the bound

$$R_t\leq O(\sqrt{\log |\mathcal{K}|\sum_{t=1}^T\mas(G_t)}$$

whenever the feedback graph is directed. If the feedback graph is undirected, $\mas(G_t)$ is replaced by $\alpha(G)$. In the informed setting, 

$$R_t \leq O(\log K\sqrt{\log KT\sum_{t=1}^T\alpha(G_t)}$$

\subsection{LP relaxations}
