\section{Algorithms}

\subsection{Exp3-SET}
The Exp3-SET is exactly as the Exp3 algorithm, except for the formation of the surrogate loss. Given in algorithm 1, instead of dividing the surrogate loss $\loss_{i,t}$ with the probability of the action $i$, it is divided by $q_{i,t}$ as defined in the analysis part; that is, the probability that the loss of action $i$ is revealed. It is thus a direct and very natural extension of the Exp3 algorithm. 

\subsection{Exp3-DOM}
Since the Exp3-DOM holds with sharp bounds in the more general case of assymmetric graphs, it also leverages more techniques. Given in algorithm 2, as seen by the superscript ${}^{(b)}$ the algorithm runs $K$ $Exp3$ algorithms and then computes an approximation of the dominating set $R_t$ using the Greedy Set Cover algorithm. The size of $R_t$ is used as an index over Exp3 algorithms and decides which probability distribution to pick an action from. Interestingly, note in line 8 that whenever the exploration parameter $\gamma$ is small little exploration is done.

Conversely, when $\gamma$ is closed to one, all the weight is put on exploration, and the weights from the previous timestep counts for nothing. Lastly, the loss is expressed as a fraction of $\gamma^{(b_t)}/R_t$ which penalises for exploration. 

\input{gsf_exp3set.tex}
\unskip
\input{gsf_exp3dom.tex}
\unskip

\subsection{Bandit Online Linear Optimization}
In this section we take a look at the main results from \citep{hazan}.
Specifically  the algorithms used in the paper are presented and elaborated on.
The main algorithm, given in algorithm 3, is composed of two prominent steps that are alternated with 
some probability. This algorithm uses reservoir sampling and the 
properties of Dikin-ellipsoids to efficiently perform online bandit linear
optimization that, as shown, can be bounded in terms of total variation.

The algorithm has three main parts: calling SimplexSample with some probability, calling EllipsoidSample with the remaining probability mass, and updating the value $x_t$ at every round. At a high level the SimplexSample is an exploration step, while the EllipsoidSample is an exploration and exploitation step.

Lines 1-4 initialises various quantities. Seen listed are the parameters of the algorithm: $\eta$ is the exploration/exploitation trade off rate, the algorithm is $\mathcal{V}$-self-concordant via $\mathcal{R}$, and a size parameter $k$ denotes the size of the reservoir kept for each of coordinate. The variable $x$ is also initialised to be the index of the minimum point in $\mathcal{R}(x)$; the estimate for the mean loss of each action, zero.

Then, at time $t$ some action is taken in a for-loop until time $T$. At each round the algorithm has the choice of exploring alone with a SimpleSample or exploring and exploiting with an EllipsoidSample step. The proportion of time spent in either of these tasks is determined by the parameter $\eta$ and the stage of the optimization as denoted by $t$. The reservoir of size $k$ is also taken into account due to how it affects the accuracy of our estimates of the mean losses being estimated. More generally, let the proportion of time spent exploring grows smaller as time goes on (time spent in SimplexSample) and the proportion of time spent exploring and exploiting increases in later stages (time spent in EllipsoidSample).

Clearly, the estimate of the mean $\tilde{\mu}$ of each coordinate computed by the reservoir sampling procedure only gets updated during the SimplexSample call, but only gets used in the EllipsoidSample call. Intuitively this shpws that exploitation is akin to using the information of losses that have been collected for each action while exploration involves gathering this information. In line with that $\tilde{f_t}$ is not updated when in the SimplexSample step, but only in the EllipsoidSample step. 

%algorithms 
\input{bbb_mainalgo.tex}
\unskip
\input{bbb_simplexsample.tex}
\unskip
\input{bbb_ellipsoidsample.tex}
\unskip

%TODO clarify what the $i_t$ step is doing 
%TODO clarify why the f_t is not updated in more detail

\subsubsection{SimplexSample}
Turning our attention to the SimplexSample procedure, its job is to implement reservoir sampling on all the points in the feasible set. SimplexSample samples a random coordinate $i \in [n]$ uniformly, the actual sampled point $y_t$ is the corresponding vertex $\gamma e_{i_t}$. This vertex is in the scaled $n$-dimensional simplex and, by assumption, it has to be contained inside $\mathcal{K}$. The loss is immediately received as $f_t(i_t)$ after which the resevoir sample is done: if one of the slots in the reservoir for that coordinate is empty then the loss is put into that slot, otherwise a random element of the resevoir is discarded and the received loss is inserted. The discarding  is done so at random, uniformly. In this way reservoir sampling is implemented correctly and guarantees an unbiased estimate of the mean loss for that coordinate in the limit. In the return step the entire vector of means to be used in the main algorithm is handed back. Every element of that vector is the estimate of the mean of the coordinate denoted by its index, at the current round $t$.
 
%Describe the ELLIPSOID-SAMPLE PROCEDURE
\subsubsection{EllipsoidSample}
EllipsoidSample exploits the algorithm's knowledge of the adversary (loss surface) by using the estimate of the mean. This is a modification of a similar procedure outlined in \citep{abernethy}. \citep{abernethy} manages to prove that this style of sampling procedure is unbiased and has low variation with respect to the regularization which in this case corresponds to the self-concordant functions. Like in SimplexSample, a point $y_t$ is chosen, however, this time it is chosen from the endpoints of the principal axes of the Dikin ellipsoid $W_1(x_t)$ centered at $x_t$. Recall that $x_t$ was constructed to minimize the FTRL loss $\eta \sum_{\tau=1}^T\tilde{f_t^T}x +\mathcal{R}$. Since the loss minimized was in terms of the estimate of the loss vector, this is where the algorithm exploits its prior knowledge of the losses associated with the coordinates. Furthermore, the estimate of $\tilde{f_t}$ is updated at the end of EllipsoidSample to incorporate the new knowledge of the mean $\tilde{\mu_t}$ and the actual loss suffered $f_t$. It is done by adding $f_t$ to $\eta (f_{t}^Ty_t - \tilde{\mu_{t}^T}y_t)\epsilon \lambda_{it}^{\frac{1}{2}}v_{it}$.

The procedure is quite akin to the exploration-exploitation steps taken in FTRL or Exp3, but there is a lot of mathematical machinery needed to ensure that the sampled point $y_t$ will be in the feasible set and that the ellipsoid sampling procedure has nice properties that ensure correctness and efficiency. 
