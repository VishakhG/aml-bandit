The main theoretical result of \citep{hazan} combines the above concepts into a bound on the regret in terms of the total variation of the cost vectors for oblivious adversarial bandits. Particularly we are in the regime where the points ${\bf x}_t \in \mathcal{K}$ where $\mathcal{K} \in \mathbb{R}^n$ is a compact 
convex set. Let Q be an estimated upperbound on $Q_T$, then for $$\eta = \min\{\sqrt{\frac{log T}{\eta Q}}, \frac{1}{25n}\}$$ we have the following bound on regret:

\begin{equation}
  E[Regret_t] = O(n \sqrt{\mathcal{V}Q log T} + n log^2(T) + n \mathcal{V}log(T))
\end{equation}

The assumed upperbound on $Q_T$ is to simplify the exposition and is not required to carry out the proof as is shown in the paper. The main series of arguments made to demonstrate the bound can be listed as a series of lemmas. In order to show how the authors manages to pull in the total variation they are included and discussed below under the numbering provided by \citep{hazan} for reference.

\begin{clemma}{7}
	For any $u \in \mathcal{K}$
	\begin{equation}
  		E[\sum_{t=1}^T f_{t}^T(y_t - u)] \leq E[\sum_{t=1}^T \tilde{f_{t}^T}(x_t - u)] + 2nlog^2(T).
	\end{equation}
\end{clemma}

This Lemma serves to relate the expected regret of the algorithm with the cost vectors of another algorithm that plays just ${\bf x}_t$ and not the points ${\bf y}_t$ as derived from ${\bf x}_t$. This relationship links the key quantities in $\sum_{t-1}^T \tilde{{\bf f}_t^T}({\bf x}_t - {\bf u})$. Furthermore, notice that the expectations of $\tilde{t_t}$ and $\tilde{y_t}$ are ${\bf f}_t$ and ${\bf x}_t$ respectively. Thus their expected costs can be related per round as done in the lemma. The expected number of rounds grows as $O(nklog(T))$ which explains the last term in the expression: $2n\log^2(T)$. This is used to our advantage when applying typical proof techniques used in FTRL type algorithms.

\begin{clemma}{8}
	For any sequence of cost vectors $\{\tilde{f_1}, ...,\tilde{f_t}\} \in \mathbb{R}^n$ the FTRL algorithm with a $\mathcal{V}$-self concordant barrier $\mathcal{R}$ admits the following
	guarantee: for any ${\bf u} \in \mathcal{K}$ we have:

	\begin{equation}
  		\sum_{t=1}^T \tilde{F_{t}^T} ({\bf x}_t - {\bf u}) \leq sum_{t=1}^T ({\bf x}_t - {\bf x}_{t+1}) + \frac{2}{\eta}\mathcal{V} log T
	\end{equation}
\end{clemma}

This statement appeals to a commonly used technique of bounding a FTRL type algorithm by how closeness of the the succesive values ${\bf x}_t, {\bf x}_{t+1}$. Next, the the different sample steps, namely EllipsoidSample and SimplexSample, are related to the bound.

\begin{clemma}{9}
Let t be an EllipsoidSample step. Then,
	\begin{align}
  		\tilde{{\bf f}_t^T} & ({\bf x}_t - {\bf x}_{t+1})\\
  		&\leq 64 \eta n^2 ||{\bf f}_t - {\bf \mu}_t ||^2 + 64 \eta n^2 ||{\bf \mu}_t - \tilde{{\bf \mu}_t} ||^2 + 2 {\bf \mu}_t^T ({\bf x}_t - {\bf x}_{t+1})\notag.
	\end{align} 
\end{clemma}

To understand Lemma 9, turn instead to the equivalent case for SimplexSample where the analysis
is simpler and then fill in the remaining parts. Since in SimplexSample $\tilde{{\bf f}_t} = 0$ the following must be true: 
$\tilde{{\bf f_t}^T}({\bf x}_t - {\bf x}_{t+1})= 0 = 2 {\bf \mu}_t^T ({\bf x}_t - {\bf x}_{t+1})$. Thus for any SimplexSample step we get 
\begin{align*}
	\tilde{{\bf f}_t^T}({\bf x}_t - {\bf x}_{t+1}) \leq 64 \eta n ||{\bf f}_t - {\bf \mu}_t ||^2 2{\bf \mu}_t^T ({\bf x}_t - {\bf x}_{t+1})
\end{align*}

Call the set of all EllipsoidSample steps $T_E$ and sum over the time periods. Doing so yields terms that can be bounded by the inequalities for SimplexSample, which gets us the final form of Lemma 9.

Next, some facts about how the sampling procedures affect the guarantee are needed.
\begin{clemma}{10}
	\begin{equation}
  		\sum_{t=1}^T ||{\bf f}_t - {\bf \mu}_t||^2 \leq Q_T
	\end{equation}
\end{clemma}

This simply follows from using the properties of the variance of the estimators of the mean reward loss $\tilde{\mu}$ when using resevoir sampling which is shown by \citep{vitter}. Finally, the total variation is introduced into the bounds by recognizing that the following upperbound exists in the equalities shown so far.

\begin{clemma}{11}
	\begin{equation}
  		E[\sum_{t \in T_E} ||{\bf \mu}_t - \tilde{{\bf \mu}_t} ||^2] \leq \frac{\log T}{k} Q_T
	\end{equation}
\end{clemma}

This step upperbounds the succesive difference of means seen in Lemma 10 by the total variation. 
Entering some messier terrain, the hyperparameters are set to optimal values such that the bound becomes interpretable and tighter.

\begin{clemma}{12}
	\begin{equation}
  		\sum_{t=1}^T {\bf \mu}_t^T ({\bf x}_t - {\bf x}_{t+1}) \leq 2 \log(Q_T + 1) + 4
	\end{equation}
\end{clemma}

The first such optimal value inserted is for the resevoir of size $k$. Plug in the bounds from lemmas 10, 11, and 12 into the bound from lemma 4 and use the value $k=log(T)$ to obtain the simpler expression:

$\sum_{t=1}^T {\bf f}_t^T ({\bf x}_t - {\bf x}_{t+1}) \leq 128 \eta^2Q + 4\log(Q_T +1))+ 8$

Now, consider the value of the exploration-exploitation tradeoff parameter $\eta$, and assume that the value of $\eta$ is larger than some value $\frac{\log(Q_t +1)}{8 n^2 Q}$. Using this assumption (which we are in control of since $\eta$ is a hyperparameter that we can choose) gives the following upperbound on total variation  $Q_T + 1 \leq 8 \eta n^2 Q$.

Using lemmas 8 and 7, for any $u \in \mathcal{K}$ it holds that: 
\begin{align*}
	E[&\sum_{i=1}^T {\bf f}_t^T ({\bf y}_t - {\bf y}) ] \\ 
	\leq&\ 128 n^2Q + \frac{2 \mathcal{V}}{\eta} logT +  2 n\log^2(T) + 4\log(Q_T +1) + 8
\end{align*}

Then choosing a suitable value for $\eta$ that respects the assumptions made earlier about its value recovers the original regret bound:
\begin{align}
  E[\sum_{t=1}^T& f_t^T (y_t - u)] \\
  \leq&\ O(n \sqrt{\mathcal{V}Qlog T} + n \mathcal{V} log(T) + nlog^2(T))\notag.
\end{align}
